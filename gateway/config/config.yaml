# NGO-Claw Gateway Configuration
# 照搬 OpenClaw 运行配置 2026-02-13

gateway:
  host: 0.0.0.0
  port: 18790  # 避免与 OpenClaw 18789 冲突
  mode: local

ai_service:
  host: localhost
  port: 50051
  timeout: 120

telegram:
  bot_token: "8301149736:AAFl3vGRzfxLe2Iw3kmViSIyNzU34W5M3Qs"
  allow_ids: [6153003667, 6048488199]
  mode: polling
  dm_policy: allowlist
  group_policy: allowlist
  group_allow_from: []

database:
  type: sqlite
  dsn: ngoclaw.db

log:
  level: debug
  format: console

# Agent 配置 (与 OpenClaw 完全对齐)
agent:
  default_model: "bailian/qwen3-max-2026-01-23"
  default_provider: "bailian"
  workspace: "/home/none/clawd"
  ask_mode: true

  # Per-model policy overrides (optional — auto-detection covers most cases).
  # Keys are matched by substring against model ID.
  # Omitted fields use auto-detected defaults from resolveModelPolicy().
  model_policies:
    qwen:
      reasoning_format: "xml"
      thinking_tag_hint: true
      progress_interval: 15
      prompt_style: "detailed"
    minimax:
      reasoning_format: "none"
      progress_interval: 8
      prompt_style: "concise"
    claude:
      reasoning_format: "native"
      progress_interval: 0   # Claude self-terminates, no injection needed

  models:
    # Bailian (OpenClaw primary)
    - id: "bailian/qwen3-max-2026-01-23"
      alias: "qwen3-max-thinking"
      provider: "Bailian"
      description: "Qwen3 Max Thinking"
    - id: "bailian/qwen3-coder-plus"
      alias: "coder"
      provider: "Bailian"
      description: "Qwen3 Coder Plus"
    # MiniMax
    - id: "minimax/MiniMax-M2.5"
      alias: "m25"
      provider: "MiniMax"
      description: "MiniMax M2.5 (latest)"
    - id: "minimax/MiniMax-M2.1"
      alias: "Minimax"
      provider: "MiniMax"
      description: "MiniMax M2.1"
    - id: "minimax/MiniMax-M2.1-lightning"
      alias: "minimax-light"
      provider: "MiniMax"
      description: "MiniMax Lightning"

  fallback_models:
    - "minimax/MiniMax-M2.5"

  # ─── Go 内置 LLM Provider 配置 ───
  # 这些 Provider 由 Go gateway 直接调用 (不经过 Python AI-Service)
  # LLM Router 按数组顺序尝试: 先 bailian → 失败后 minimax
  # 每个 provider 字段说明:
  #   name:     唯一标识符, 对应 models 列表中 provider 字段的前缀
  #   base_url: OpenAI 兼容 API 的 endpoint (必须以 /v1 结尾)
  #   api_key:  API 密钥 (也可通过环境变量 NGOCLAW_BAILIAN_KEY 等覆盖)
  #   models:   该 provider 支持的模型 ID 列表 (含 provider/ 前缀)
  #   priority: 优先级, 数字越小优先级越高
  providers:
    - name: bailian
      base_url: "https://coding.dashscope.aliyuncs.com/v1"
      api_key: "sk-sp-fbc39e83e0a440bb954c3b85a0affe9b"
      models:
        - "bailian/qwen3-max-2026-01-23"
        - "bailian/qwen3-coder-plus"
      priority: 1

    - name: minimax
      base_url: "https://api.minimaxi.com/v1"
      api_key: "sk-cp-BVsG7gjbVdluUQfvRzQL26pPrurkcZeOo8HVH2x4_WTyPNFNutdVWM-E76VctNL86JlI38fTYp0tD9o0E35tdorCwnHEzVgpzbgW97p-iiOcwZLUEUZTHII"
      models:
        - "minimax/MiniMax-M2.5"
        - "minimax/MiniMax-M2.1"
        - "minimax/MiniMax-M2.1-lightning"
      priority: 2

  # gRPC Agent Server 端口 (VS Code Extension / SDK 用)
  grpc_port: 50052  # 50051 已被 Python AI-Service 占用

  # 运行时参数
  runtime:
    tool_timeout: 60s
    run_timeout: 10m
    sub_agent_timeout: 3m
    sub_agent_max_steps: 25
    max_token_budget: 180000
    concurrent_tools: true
    max_retries: 3          # LLM 调用失败重试次数 (指数退避: 2s, 4s, 8s)
    retry_base_wait: 2s     # 重试基础等待 (每次翻倍)

  # 防护栏
  guardrails:
    context_max_tokens: 180000
    context_warn_ratio: 0.7
    context_hard_ratio: 0.85
    loop_detect_window: 10
    loop_detect_threshold: 5
    cost_guard_enabled: true

  # 工具注册表
  tools:
    registry:
      - name: web_search
        backend: command
        command: "python3 ~/tools/web_search.py"
        enabled: true
        aliases:
          claude: ["WebSearch", "web_search", "websearch"]
          gemini: ["web_search", "google_search"]
          openai: ["web_search"]
          minimax: ["web_search", "search"]

      - name: read_file
        backend: go
        handler: "builtin:read_file"
        enabled: true
        aliases:
          claude: ["ReadFile", "read_file"]
          gemini: ["read_file"]

      - name: execute_command
        backend: go
        handler: "builtin:execute_command"
        enabled: true
        aliases:
          claude: ["ExecuteCommand", "Bash", "bash", "execute_command"]
          gemini: ["execute_command", "run_command"]

  # 压缩参数
  compaction:
    message_threshold: 30
    token_threshold: 30000
    keep_recent: 10
    summary_max_tokens: 1000
    pre_flush_to_memory: true

  # MCP
  mcp:
    servers: []

# 向量记忆系统 (Ollama Embedding)
memory:
  enabled: true
  ollama_url: "http://192.168.31.55:11333"
  embed_model: "qwen3-embedding"
  store_path: "~/.ngoclaw/memory/lancedb"
  store_type: "lancedb"

# 全局 Python 环境 (conda / venv 根目录, 所有 skill/tool 均走此环境)
python_env: "/home/none/miniconda3/envs/claw"
