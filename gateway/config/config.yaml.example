# NGO-Claw Gateway Configuration Example
# 复制此文件为 ~/.ngoclaw/config.yaml 后按需修改
# 首次启动会自动生成默认配置，此文件仅供参考

gateway:
  host: 0.0.0.0
  port: 18790
  mode: local            # local | production

ai_service:
  host: localhost
  port: 50051
  timeout: 120           # gRPC 超时秒数

telegram:
  bot_token: ""          # @BotFather 获取
  allow_ids: []          # 允许的 Telegram 用户 ID
  mode: polling          # polling | webhook
  dm_policy: allowlist   # open | allowlist | disabled
  group_policy: allowlist

database:
  type: sqlite
  dsn: ngoclaw.db

log:
  level: info            # debug | info | warn | error
  format: console        # console | json

agent:
  default_model: ""                 # 如 "bailian/qwen3-max-2026-01-23"
  default_provider: ""              # 如 "bailian"
  workspace: ""                     # 工作目录，如 "/home/user/projects"
  max_iterations: 50                # Agent 最大步数
  ask_mode: true                    # 是否开启确认模式

  # 可用模型列表 (示例)
  models:
    - id: "provider/model-name"
      alias: "short-name"
      provider: "ProviderName"
      description: "Model description"

  # 容灾备选模型链
  fallback_models:
    - "provider/fallback-model"

  # Go 内置 LLM Provider 配置
  # LLM Router 按数组顺序尝试: 先 primary → 失败后 fallback
  providers:
    - name: primary
      base_url: "https://api.example.com/v1"    # OpenAI 兼容 endpoint
      api_key: "sk-your-api-key-here"           # ⚠️ 替换为真实 key
      models:
        - "primary/model-a"
        - "primary/model-b"
      priority: 1

    - name: fallback
      base_url: "https://api.fallback.com/v1"
      api_key: "sk-your-fallback-key"           # ⚠️ 替换为真实 key
      models:
        - "fallback/model-a"
      priority: 2

  # gRPC Agent Server 端口
  grpc_port: 50052

  # 运行时参数
  runtime:
    tool_timeout: 60s               # 单个工具执行超时
    run_timeout: 10m                # 单次 Run 最大时长
    sub_agent_timeout: 3m           # 子 Agent 超时
    sub_agent_max_steps: 25         # 子 Agent 最大步数
    max_token_budget: 180000        # Token 预算上限
    concurrent_tools: true          # 是否并发执行工具
    max_retries: 3                  # LLM 调用失败重试次数 (指数退避)
    retry_base_wait: 2s             # 重试基础等待 (2s → 4s → 8s)

  # 防护栏
  guardrails:
    context_max_tokens: 180000      # 上下文窗口大小
    context_warn_ratio: 0.7         # 警告阈值 (70%)
    context_hard_ratio: 0.85        # 强制压缩阈值 (85%)
    loop_detect_window: 10          # 循环检测滑动窗口
    loop_detect_threshold: 5        # 同一工具连续 N 次视为循环
    cost_guard_enabled: true        # 启用成本保护

  # 工具注册表 (示例)
  tools:
    registry:
      - name: web_search
        backend: command             # go | python | command | grpc
        command: "python3 ~/tools/web_search.py"
        enabled: true
        aliases:                     # 不同 provider 的工具名映射
          claude: ["WebSearch"]
          gemini: ["web_search"]

      - name: read_file
        backend: go
        handler: "builtin:read_file"
        enabled: true

      - name: execute_command
        backend: go
        handler: "builtin:execute_command"
        enabled: true

  # 上下文压缩参数
  compaction:
    message_threshold: 30           # 消息数触发阈值
    token_threshold: 30000          # Token 数触发阈值
    keep_recent: 10                 # 保留最近 N 条
    summary_max_tokens: 1000        # 摘要最大 token
    pre_flush_to_memory: true       # 压缩前写关键事实到向量库

  # MCP (Model Context Protocol) 服务器
  mcp:
    servers: []
    # - name: "example"
    #   endpoint: "http://localhost:8080"
    #   enabled: true

# 向量记忆系统
memory:
  enabled: false
  ollama_url: ""                    # 如 "http://localhost:11434"
  embed_model: ""                   # 如 "qwen3-embedding"
  store_path: "~/.ngoclaw/memory/lancedb"
  store_type: "lancedb"             # lancedb | memory

# 全局 Python 环境 (conda / venv)
python_env: ""                      # 如 "/home/user/miniconda3/envs/claw"
